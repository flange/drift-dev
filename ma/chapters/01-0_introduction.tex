\section{Introduction}
Alongside the advent of multi-core CPU architectures
and increased local storage capacities, large scale distributed
systems based on commodity hardware, accommondated for in special
facilities like data centers, have proven to be a viable tool for
dealing with the ever increasing demand of compute power and
storage.

One of the largest drivers of this demand is of course
the modern web, including not only all the data accumulation
that is taking place on social networks and social media but also
providing different kinds of large scale services and platforms
like maps and navigations but also e-commerce platforms which
today include music and video streaming platforms like
\textit{Spotify}, \textit{Netflix} or \textit{Amazon Prime Music} or
\textit{Amazon Prime Video} or even transportation-on-demand
platforms like \textit{Uber}.

Additionally, the advent of artificial intelligence and
machine learning created its own demand for ever increasing
large scale neural networks and training data sets. A progression
that is deeply coupled with the products of the modern web,
providing not only recommondation or prediction engines for
the users but also scheduling and load balancing decisions
inside the systems, perfectly adapted to the given workload,
which allow to fully utilize a given system to capacity without
human intervention.
\newline

Therefore, in order to provide systems that allow to scale to millions
or even billions of users, either in terms of storage, computing,
training neural networks or simply content delivery and response times,
large scale distributed systems have been constructed which pose new
system design challenges not only regarding fault tolerance, network failure
and fail over strategies but also in terms of simply how to
describe, express and comprehend these vast and infinitely
complex systems.







